{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnHoCeVhDHLB0HqfmfmPtx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/connectchayan/ViBe/blob/main/MICRO_LLM_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfpryfIO0zEc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a126e98"
      },
      "source": [
        "# Task\n",
        "Build a RAG system using a micro LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a13058d5"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install libraries such as `transformers`, `datasets`, `torch`, and `langchain` that will be used to build the RAG system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da0868a0"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2020558d",
        "collapsed": true
      },
      "source": [
        "%pip install transformers datasets torch langchain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79e7db8d"
      },
      "source": [
        "## Load a micro llm\n",
        "\n",
        "### Subtask:\n",
        "Load a pre-trained micro LLM model from the `transformers` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c23879b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classes from the `transformers` library and load a pre-trained micro LLM model and its tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6977a0c8"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Specify the name of a pre-trained micro LLM model\n",
        "model_name = \"distilgpt2\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Tokenizer for {model_name} loaded successfully.\")\n",
        "print(f\"Model {model_name} loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4fd855d"
      },
      "source": [
        "## Load and process data\n",
        "\n",
        "### Subtask:\n",
        "Load the data that will be used as the knowledge base for the RAG system. Process the data into a suitable format for the LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "495be38c"
      },
      "source": [
        "**Reasoning**:\n",
        "Load a dataset from the `datasets` library, inspect its structure, select relevant columns, and process it into a suitable format for a RAG system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13bf606f"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load a suitable dataset for RAG\n",
        "# Using 'squad' as it's a question-answering dataset\n",
        "dataset = load_dataset(\"squad\")\n",
        "\n",
        "# Inspect the dataset structure\n",
        "print(dataset)\n",
        "\n",
        "# Select relevant columns and process data\n",
        "# For SQuAD, we can use 'context', 'question', and 'answers'\n",
        "# We'll focus on 'context' as the knowledge base text\n",
        "def process_squad_dataset(examples):\n",
        "    # Concatenate context and question for potential use, or just keep context\n",
        "    # For a simple RAG knowledge base, context is sufficient\n",
        "    return {\"text\": examples[\"context\"]}\n",
        "\n",
        "# Apply the processing function to the dataset\n",
        "processed_dataset = dataset.map(process_squad_dataset, remove_columns=['id', 'title', 'question', 'answers'])\n",
        "\n",
        "# Inspect the processed dataset\n",
        "print(processed_dataset)\n",
        "print(processed_dataset['train'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cad14b5b"
      },
      "source": [
        "## Build the rag system\n",
        "\n",
        "### Subtask:\n",
        "Build the RAG system using the loaded LLM and processed data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d3230ef"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classes from langchain, create embeddings for the processed data, build a vector store from the embeddings, and set up the RetrievalQA system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePbReV7O14TV"
      },
      "source": [
        "%pip install langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2dcde71"
      },
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# 2. Create embeddings for the text data\n",
        "# Using a smaller, faster sentence transformer model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Extract text from the processed dataset\n",
        "texts = [example['text'] for example in processed_dataset['train']]\n",
        "\n",
        "# 3. Create a vector store from the embeddings and text data\n",
        "# Using FAISS for efficient similarity search\n",
        "vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "# 4. Set up a retrieval mechanism\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 5. Combine the loaded micro LLM, the retrieval mechanism\n",
        "# Create a HuggingFace pipeline for the loaded model\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create the RetrievalQA system\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "print(\"RAG system (RetrievalQA chain) created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6208571c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `langchain_community` module was not found. This is likely due to a version mismatch or the module being in a different location in the installed langchain package. The fix is to install `langchain-community`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06dc1edc"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `langchain-community` is installed, retry the previous command to import necessary classes, create embeddings, build the vector store, and set up the RetrievalQA system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bfb7f9f"
      },
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# 2. Create embeddings for the text data\n",
        "# Using a smaller, faster sentence transformer model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Extract text from the processed dataset\n",
        "# Taking a smaller subset for demonstration to avoid excessive memory usage\n",
        "sample_size = 1000\n",
        "texts = [example['text'] for example in processed_dataset['train'].select(range(sample_size))]\n",
        "\n",
        "# 3. Create a vector store from the embeddings and text data\n",
        "# Using FAISS for efficient similarity search\n",
        "vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "# 4. Set up a retrieval mechanism\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 5. Combine the loaded micro LLM, the retrieval mechanism\n",
        "# Create a HuggingFace pipeline for the loaded model\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create the RetrievalQA system\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "print(\"RAG system (RetrievalQA chain) created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81eca144"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `faiss` package was not found. The traceback suggests installing either `faiss-gpu` or `faiss-cpu`. Since we are not guaranteed to have a GPU environment, `faiss-cpu` is the safer choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLoMpL4x2cI6"
      },
      "source": [
        "%pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cd5149"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `faiss-cpu` is installed, retry the command to import necessary classes, create embeddings, build the vector store, and set up the RetrievalQA system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d1cc613"
      },
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# 2. Create embeddings for the text data\n",
        "# Using a smaller, faster sentence transformer model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Extract text from the processed dataset\n",
        "# Taking a smaller subset for demonstration to avoid excessive memory usage\n",
        "sample_size = 1000\n",
        "texts = [example['text'] for example in processed_dataset['train'].select(range(sample_size))]\n",
        "\n",
        "# 3. Create a vector store from the embeddings and text data\n",
        "# Using FAISS for efficient similarity search\n",
        "vectorstore = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "# 4. Set up a retrieval mechanism\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# 5. Combine the loaded micro LLM, the retrieval mechanism\n",
        "# Create a HuggingFace pipeline for the loaded model\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=100\n",
        ")\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Create the RetrievalQA system\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "print(\"RAG system (RetrievalQA chain) created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d9d8a1c"
      },
      "source": [
        "## Test the rag system\n",
        "\n",
        "### Subtask:\n",
        "Test the RAG system with some example queries to ensure it is working correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ede624d5"
      },
      "source": [
        "**Reasoning**:\n",
        "Define example queries and use the qa_chain to get responses for each query.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daf5093a"
      },
      "source": [
        "# Define example queries\n",
        "example_queries = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is the purpose of the lymphatic system?\",\n",
        "    \"Who was the first man on the moon?\",\n",
        "    \"What is the function of photosynthesis?\",\n",
        "    \"What is machine learning?\"\n",
        "]\n",
        "\n",
        "# Get responses for each query using the qa_chain\n",
        "for query in example_queries:\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response['result']}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d918fd40"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt resulted in an IndexError because the input sequence length exceeded the model's maximum length. To fix this, I need to adjust the `max_new_tokens` parameter in the HuggingFace pipeline to a smaller value to prevent generating sequences that are too long. I will also re-run the testing with the example queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5txOwVQY3GrA"
      },
      "source": [
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# Re-create the HuggingFace pipeline with a smaller max_new_tokens\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=50 # Reduced from 100\n",
        ")\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Re-create the RetrievalQA system with the updated LLM\n",
        "# The retriever remains the same\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "print(\"RAG system (RetrievalQA chain) updated with reduced max_new_tokens.\")\n",
        "\n",
        "# Re-run the testing with example queries\n",
        "example_queries = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is the purpose of the lymphatic system?\",\n",
        "    \"Who was the first man on the moon?\",\n",
        "    \"What is the function of photosynthesis?\",\n",
        "    \"What is machine learning?\"\n",
        "]\n",
        "\n",
        "# Get responses for each query using the qa_chain\n",
        "for query in example_queries:\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response['result']}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "463da08e"
      },
      "source": [
        "**Reasoning**:\n",
        "The IndexError persists even after reducing `max_new_tokens`. This indicates the issue might be with the combined length of the prompt (context + query) exceeding the model's limit. To address this, I will limit the number of documents retrieved by the retriever to reduce the context size provided to the LLM. I will set `search_kwargs` to limit the retrieval to a smaller number of documents and then re-run the tests.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC9yICow3Km-"
      },
      "source": [
        "# Set up a retrieval mechanism with limited search results\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1}) # Limit to retrieving only 1 document\n",
        "\n",
        "# Re-create the RetrievalQA system with the updated retriever\n",
        "# The LLM pipeline remains the same as updated in the previous step\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "print(\"RAG system (RetrievalQA chain) updated with limited document retrieval.\")\n",
        "\n",
        "# Re-run the testing with example queries\n",
        "example_queries = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is the purpose of the lymphatic system?\",\n",
        "    \"Who was the first man on the moon?\",\n",
        "    \"What is the function of photosynthesis?\",\n",
        "    \"What is machine learning?\"\n",
        "]\n",
        "\n",
        "# Get responses for each query using the qa_chain\n",
        "for query in example_queries:\n",
        "    response = qa_chain.invoke({\"query\": query})\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Response: {response['result']}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cde5d8dd"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The necessary libraries (`transformers`, `datasets`, `torch`, `langchain`, `langchain-community`, `faiss-cpu`) were successfully installed.\n",
        "*   A pre-trained micro LLM model, \"distilgpt2\", and its tokenizer were successfully loaded from the `transformers` library.\n",
        "*   The SQuAD dataset was successfully loaded using the `datasets` library and processed to extract the 'context' information into a 'text' column.\n",
        "*   A RAG system was successfully built using `RetrievalQA` from `langchain`, incorporating a `HuggingFaceEmbeddings` model, a `FAISS` vector store created from a subset of the processed data, and a `HuggingFacePipeline` for the loaded micro LLM.\n",
        "*   Initial testing of the RAG system resulted in an `IndexError`, which was resolved by limiting the number of retrieved documents to one (`search_kwargs={\"k\": 1}`) to manage the input length for the micro LLM.\n",
        "*   The RAG system was able to process queries after the input length issue was addressed, but the quality of the generated answers was poor, highlighting the limitations of using a micro LLM and a dataset like SQuAD for generative QA.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Consider using a larger, more capable language model if improved answer quality is required, or fine-tune the current micro LLM on a task-specific dataset.\n",
        "*   Explore alternative datasets or pre-processing techniques that are better suited for generative question answering with a RAG system.\n"
      ]
    }
  ]
}